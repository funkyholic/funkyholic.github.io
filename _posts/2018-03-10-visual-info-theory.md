---
layout: post
title:  Visual Infomation Theory
date:   2018-03-10 20:36:28 +0800
categories: tech test
desc: translation of VIT
---




# 可视化信息理论 Visual Information Theory

本文翻译&转述自论文: [http://colah.github.io/posts/2015-09-Visual-Information/](http://colah.github.io/posts/2015-09-Visual-Information/)

## 可视化概率分布
在一个城市，我们不妨假设有75%时间是晴天，那么会有概率分布:
晴天75%，雨天25%。
很多时候我会穿毛衣，假设我有38%时间穿毛衣，另外62%时间穿T恤。

假设穿不穿毛衣和下不下雨是相互独立事件，因此我们可以同时在不同维度上可视化两者的概率:  

![](http://colah.github.io/posts/2015-09-Visual-Information/img/prob-2D-independent-rain.png)


假设穿不穿毛衣和下不下雨是相关的，那么两者的概率就会相互影响:  

![](http://colah.github.io/posts/2015-09-Visual-Information/img/prob-2D-dependant-rain-squish.png)

显然这种可视化表达难以描述清晰的数据，鉴于此我们可以使用条件概率。
比如在晴天，我们穿毛衣的概率是25%，穿T恤的概率是75%。在雨天我们穿T恤的概率是25%，穿毛衣的概率是75%。
那么我们可以得出概率分布:
![](http://colah.github.io/posts/2015-09-Visual-Information/img/prob-2D-factored-rain-arrow.png)


## 题外话: 辛普森悖论 Simpson's paradox

极端非直观统计现象。

有两种治疗肾结石方法A和B，现在有一组病人，其中一半接受A方法治疗，另一半接受B方法治疗。统计结果表明接受A方法治疗的病人的存活率比接受B治疗低。肾结石病又分为大和小两种情况，得小肾结石的病人接受A方法治疗比接受B方法更容易存活，得大肾结石的病人也是接受A方法治疗比接受B方法更容易存活。

单独考虑条件，那么显然A方法的存活率高，而总体考虑确实B方法的存活率高，到底该相信A还是B，这存在悖论。

造成这种现象的原因是，统计样本分配不均匀。接受A方法治疗的病人中大多数都是大肾结石的，而接受B方法治疗的病人中大多数都是小肾结石的。而当然得小肾结石的比得大肾结石的存活率高。


## 编码 Code
我朋友Bob喜欢动物，并且他只会说4个单词: dog, cat, fish, bird。
假设他把这4个单词进行二进制编码:
dog = 00, cat = 01, fish = 10, bird = 11。
并且Bob通过编码与我交流。
然而信息传输成本是很高的，假设Bob经常说dog，那么我每次都要支付00这两个bit的信息传输成本。


### 可变长编码
因此需要考虑，对于高频信息尽量减少传输的字符。
现在我们统计Bob说这4种单词的频率:
dog = 50%, cat = 25%, fish = 12.5%, bird = 12.5%

均长编码:
dog = 00, cat = 01, fish = 10, bird = 11。
其平均传输字符为 (50%+25%+12.5%+12.5%)*2 = 2

变长编码:
dog = 0, cat = 10, fish = 110, bird = 111。
其平均传输字符为 50%\*1+25%\*2+12.5%\*3+12.5%*3 = 1.75
均值1.75又称为熵。


### 变长编码规则
为何变长编码要这么编:
dog = 0, cat = 10, fish = 110, bird = 111。
这个需要考虑到解码的问题。

假设我们传输信息dog,cat,fish,dog，那么我们把字符串拼接起来传输 → 0101100
解码的时候必须可以分割出唯一的字符串0,10,110,0。

如果我们编码的时候把两个信息编码成0和01，那么在解码01的时候就会出现歧义，到底是解码成0还是01呢？

变长编码规则:
对于已有的一个编码，其他头部包含此编码的更长的字符串不可以成为编码。
Prefix property: 任意两个编码，不可能存在一个编码是另一个的前缀。
如果用了01作为编码，那么不能用0。如果用了001为编码，那么不能用0和00。

因此有些编码短了，必定有些编码长了。

### 最佳编码
编码开销，
如果编码长度n = 1，比如将一个信息编码成字符串"0"，那么将有50%的字符串不能被用来编码，因为有50%的字符串是以0开头的。同样如果n = 2，比如将一个信息编码成"01"，那么将有25%的字符串不能用。
Cost = 1/(2^(n))

策略:
如果一个信息出现几率是50%，那么我们用50%的Cost对其编码。如果出现几率1%，那么我们用1%的Cost编码。

计算长度:
信息x出现的概率 == 其开销
Cost = p(x)
计算对x进行编码的长度$n_x$
$n_x = log_2(1/Cost)$

对于一种概率分布，在最佳编码情况下，传输要用到的平均字符长度，称为熵entropy，记为H(p)。
H(p) = $\Sigma_x p(x)log_2(1/p(x))$
也就是说不论怎么编码，都至少平均传输H(p)字符串长度。

### 交叉熵 cross-entropy
现在有个Alice，她也说dog,cat,fish,bird。
但是频率与Bob不同:
dog = 12.5%, cat = 50%, fish = 25%, bird = 12.5%

如果Alice与Bob通信采用Bob的编码方式，那么其平均传输长度为 
12.5%\*1 + 50%\*2 + 25%\*3 + 12.5\*3 = 2.25
显然不是最佳的

对于两种概率分布p和q，q对p的平均通信传输长度是指分布概率q用p的最佳编码来传输信息所需要传输的平均字符串长度，称为交叉熵Cross-entropy，记为$H_p(q)$。
$H_p(q) = \Sigma_xq(x)log_2(1/p(x))$



交叉熵的意义:
p用q的编码传输和q用p的编码传输平均长度是不一样的。

Bob用Alice的最佳编码传输，平均长度为2.375
Alice用Bob的最佳编码传输，平均长度为2.25

概率分布p与q差别越大，交叉熵$H_q(p)$与p的熵差别也会越大。同理交叉熵$H_p(q)$与q的熵差别也会越大。这个差别被称为Kullback–Leibler divergence，记为$D_q(p)$。
$D_q(p) = Hq(p) - H(p)$


## 分数位编码
设a出现概率71%，b出现概率29%
根据之前计算a应该用0.5位编码，b用1.7位编码，因此平均传输长度为71%\*0.5+29%\*1.7 = 0.85
但是实际传输应该用整数，因此a用1位编码，b用1位编码，实际平均传输为1位。
如果我们分别传输2次信息，那么2次总共消耗实际传输为2位。

我们可以把2次传输合并成一次传输，那么总共会出现aa = 50%, ab = ba = 21%, bb = 8%。
根据计算:
aa = 1bit, ab = ba = 2.3bits, bb = 3.5bits
实际传输四舍五入并分配:
aa = 1bit, ab = 2bits, ba = 3bits, bb = 3bits
平均传输长度为1.8
也就是说单次传输平均每次传输0.9个bit
这个值会随着合并传输越多，越来越趋近于0.85
可见合并传输比单独传输好。
合并传输会使得编码变长，如a只需要0.5bits，aa需要1bit。



